
<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Ke Xian</title>
	<meta content="Ke Xian, KexianHust.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="KeXian" style="float: left; padding-left: .01em; height: 140px;" src="PHOTO_KEXIAN.jpg" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Ke Xian</span><br />
<span><strong>Email  </strong>: kxian[at]hust[dot]edu[dot]cn</span> <br /> 
<span>I am now a lecturer at the School of Electronic Information and Communications, Huazhong University of Science and Technology. I was a Research Fellow at <a href='https://www.ntu.edu.sg/s-lab'>S-Lab</a>, Nanyang Technological University, working with <a href='https://guosheng.github.io/'>Prof. Guosheng Lin.</a> </span>
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>(<a href='https://scholar.google.com/citations?user=UbK_AGwAAAAJ&hl=zh-CN'>Google scholar</a>)</h2>
<div class="paper">

I got my Ph.D. degree and B.Sc. degree at the Huazhong University of Science and Technology, supervised by Prof. <a http://faculty.hust.edu.cn/caozhiguo1/en/index.htm'>Zhiguo Cao</a>. From Aug. 2016 to Sep. 2017, I was a visiting student at Prof. <a href='https://cshen.github.io/'>Chunhua Shen</a>'s group at the University of Adelaide, Australia. From Mar. 2019 to Jun. 2019, I was an intern at Adobe Research, working with Dr. <a href='https://jimmie33.github.io/'>Jianming Zhang</a>, Dr. <a href='http://www.oliverwang.info/'>Oliver Wang</a>, Dr. <a href='https://mai-t-long.com/'>Long Mai</a>, and Dr. <a href='https://sites.google.com/site/zhelin625/'>Zhe Lin.</a> My research primarily centers on computer vision and computational photography, such as robust depth estimation, neural generation/rendering/editing. 

<br> <br>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
    <li> Dec, 2023: One paper about robust monocular depth estimation is accepted by IJCV. Code, model, and data will be released at <a href='https://github.com/KexianHust/Robust-MonoDepth'>here</a>.</li>
    <li> Dec, 2023: One paper about motion prediction is accepted at AAAI 2024. </li>
    <li> Aug, 2023: One paper about robust video depth estimation is accepted by T-MM. Please refer to our <a href='https://github.com/KexianHust/ViTA'>project homepage</a> for details.</li>
    <li> Jul, 2023: Three papers are accepted at MM 2023. </li>
    <li> Jul, 2023: One paper about video depth estimation is accepted at ICCV 2023. </li>
    <li> Jun, 2023: Our team is the Winner of the track 'Bokeh Effect Transformation' at NITRE 2023 (CVPR 2023). </li>
    <li> Mar, 2023: One paper about 3D cinemagraphy is accepted at CVPR 2023. Please refer to our <a href='https://xingyi-li.github.io/3d-cinemagraphy/'>project homepage</a> for details. </li>
      
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publications (Selected)</h2>

<div class="paper" id="robo-monodepth"><img class="paper" src="papers/robo-monodepth.png" title="Towards Robust Monocular Depth Estimation: A New Baseline and Benchmark" />
<div> <strong>Towards Robust Monocular Depth Estimation: A New Baseline and Benchmark
</strong><br />
<strong>Ke Xian</strong>, Zhiguo Cao, Chunhua Shen, and Guosheng Lin. <br />
International Journal of Computer Vision (IJCV), 2024, IF=19.5,
<a href=''>[PDF]</a>
<a href='https://github.com/KexianHust/Robust-MonoDepth'>[code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="vita"><img class="paper" src="papers/vita.png" title="ViTA: Video Transformer Adaptor for Robust Video Depth Estimation" />
<div> <strong>ViTA: Video Transformer Adaptor for Robust Video Depth Estimation
</strong><br />
<strong>Ke Xian</strong>, Juewen Peng, Zhiguo Cao, Jianming Zhang, and Guosheng Lin. <br />
IEEE Transactions on Multimedia (T-MM), 2023, IF=7.3,
<a href='https://ieeexplore.ieee.org/abstract/document/10233038/'>[PDF]</a>
<a href='https://kexianhust.github.io/ViTA/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="nvds"><img class="paper" src="papers/nvds.png" title="Neural Video Depth Stabilizer" />
<div> <strong>Neural Video Depth Stabilizer
</strong><br />
Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, <strong>Ke Xian*</strong>, and Guosheng Lin. (*corresponding author)<br />
ICCV 2023,
<a href='https://arxiv.org/abs/2307.08695'>[PDF]</a>
<a href='https://raymondwang987.github.io/NVDS/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>
	
<div class="paper" id="cinemagraphy"><img class="paper" src="papers/cinemagraphy.png" title="3D Cinemagraphy from a Single Image" />
<div> <strong>3D Cinemagraphy from a Single Image
</strong><br />
Xingyi Li, Zhiguo Cao, Huiqiang Sun, Jianming Zhang, <strong>Ke Xian*</strong>, and Guosheng Lin. (*corresponding author)<br />
CVPR 2023,
<a href='https://arxiv.org/abs/2303.05724'>[PDF]</a>
<a href='https://xingyi-li.github.io/3d-cinemagraphy/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="make-it-4d"><img class="paper" src="papers/make-it-4d.png" title="Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image" />
<div> <strong>Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image
</strong><br />
Liao Shen, Xingyi Li, Huiqiang Sun, Juewen Peng, <strong>Ke Xian*</strong>, Zhiguo Cao, and Guosheng Lin. (*corresponding author)<br />
ACM MM 2023,
<a href='https://dl.acm.org/doi/pdf/10.1145/3581783.3612033'>[PDF]</a>
<a href='https://github.com/leoShen917/Make-It-4D'>[Code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="bokehme"><img class="paper" src="papers/bokehme.jpeg" title="BokehMe: When Neural Rendering Meets Classical Rendering" />
<div> <strong>BokehMe: When Neural Rendering Meets Classical Rendering
</strong><br />
Juewen Peng, Zhiguo Cao, Xianrui Luo, Hao Lu, <strong>Ke Xian*</strong>, and Jianming Zhang. (*corresponding author)<br />
CVPR 2022 Oral,
<a href='https://github.com/JuewenPeng/BokehMe/blob/main/pdf/BokehMe.pdf'>[PDF]</a>
<a href='https://juewenpeng.github.io/BokehMe/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="mpib"><img class="paper" src="papers/mpib.jpeg" title="MPIB: An MPI-Based Bokeh Rendering Framework for Realistic Partial Occlusion Effects" />
<div> <strong>MPIB: An MPI-Based Bokeh Rendering Framework for Realistic Partial Occlusion Effects
</strong><br />
Juewen Peng, Jianming Zhang, Xianrui Luo, Hao Lu, <strong>Ke Xian*</strong>, and Zhiguo Cao. (*corresponding author)<br />
ECCV 2022,
<a href='https://github.com/JuewenPeng/MPIB/blob/main/pdf/MPIB.pdf'>[PDF]</a>
<a href='https://juewenpeng.github.io/MPIB/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="mfm"><img class="paper" src="papers/mfm.png" title="Less is More: Consistent Video Depth Estimation with Masked Frames Modeling" />
<div> <strong>Less is More: Consistent Video Depth Estimation with Masked Frames Modeling
</strong><br />
Yiran Wang, Zhiyu Pan, Xingyi Li, Zhiguo Cao, <strong>Ke Xian*</strong>, and Jianming Zhang. (*corresponding author)<br />
ACM MM 2022,
<a href='https://arxiv.org/abs/2208.00380'>[PDF]</a>
<a href='https://github.com/RaymondWang987/FMNet'>[Code]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="hrwsi"><img class="paper" src="papers/hrwsi.png" title="Structure-Guided Ranking Loss for Single Image Depth Prediction" />
<div> <strong>Structure-Guided Ranking Loss for Single Image Depth Prediction
</strong><br />
<strong>Ke Xian</strong>, Jianming Zhang, Oliver Wang, Long Mai,  Zhe Lin, and Zhiguo Cao. <br />
CVPR 2020,
<a href='https://openaccess.thecvf.com/content_CVPR_2020/html/Xian_Structure-Guided_Ranking_Loss_for_Single_Image_Depth_Prediction_CVPR_2020_paper.html'>[PDF]</a>
<a href='https://kexianhust.github.io/Structure-Guided-Ranking-Loss/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="redweb"><img class="paper" src="papers/redweb.png" title="Monocular Relative Depth Perception with Web Stereo Data Supervision" />
<div> <strong>Monocular Relative Depth Perception with Web Stereo Data Supervision
</strong><br />
<strong>Ke Xian</strong>, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. <br />
CVPR 2018,
<a href='https://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_Monocular_Relative_Depth_CVPR_2018_paper.pdf'>[PDF]</a>
<a href='https://sites.google.com/site/redwebcvpr18/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>

</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professional activities</h2>
<div class="paper">
<ul>
<p><strong><font size="5"> Journals</font></p></strong>
<p><font size="5">Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</font></p>
<p><font size="5">Transaction on Image Processing(TIP)</font></p>
<p><font size="5"> IEEE Transactions on Circuits and Systems for Video Technology(TCSVT)</font></p>
<p><font size="5"> IEEE Transactions on Geoscience and Remote Sensing(TGRS)</font></p>
<p><strong><font size="5"> Conferences</font></p></strong>
<p><font size="5">CVPR, ICCV, ECCV, AAAI, MM, etc.</font></p>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Competitions</h2>
<div class="paper">
<ul>
<li><strong>Winner award </strong> in NTIRE 2023 Bokeh Effect Transformation Challenge. </li>
<li><strong>Runner-up award </strong> in AIM 2020 Challenge on Rendering Realistic Bokeh.</li>
<li><strong>Runner-up award</strong> in Mobile AI Challenge 2021 on Monocular Depth Estimation.</li>
<li><strong>Runner-up award</strong> in Robust Vision Challenge 2020 on Monocular Depth Prediction.</li>
<li><strong>Runner-up award</strong> in Robust Vision Challenge 2018 on Monocular Depth Prediction.</li>
<li><strong>Champion</strong> in Renesas MCU Car Rally 2012.</li>

</ul>
<div class="spanner"></div>
</div>
</div>
</div>


<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 18th Dec, 2023</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=qkK_Y-Muc7L25Q0vp4-FjtwW7_ZkilQHRMuUa8F4VSU&cl=ffffff&w=a"></script>

</body>
</html>
